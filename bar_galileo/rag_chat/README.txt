===========================================
  RAG CHAT - Sistema de Q&A sobre Documentos
===========================================

Retrieval-Augmented Generation (RAG) para responder preguntas
bas√°ndose en documentos (manuales de usuario, PDFs, etc.).

*** IMPORTANTE: ESTE C√ìDIGO EST√Å CREADO PERO NO INSTALADO ***
*** SIGUE LOS PASOS DE INSTALACI√ìN M√ÅS ABAJO ***

===========================================
¬øQU√â ES RAG Y C√ìMO FUNCIONA?
===========================================

RAG = Retrieval-Augmented Generation

NO es un chat normal. Es un sistema que:

1. INDEXA documentos (tu manual de usuario en PDF)
2. Cuando preguntas algo, BUSCA en esos documentos
3. GENERA una respuesta basada en lo que encontr√≥

EJEMPLO:
- Usuario: "¬øC√≥mo crear un empleado?"
- Sistema:
  a) Busca en el PDF fragmentos sobre "crear empleado"
  b) Encuentra 3 p√°rrafos relevantes (p√°ginas 5, 8, 12)
  c) Se los pasa a Google Gemini como contexto
  d) Gemini responde bas√°ndose SOLO en esos fragmentos
  e) Usuario recibe respuesta + fuentes (p√°ginas)

===========================================
ARQUITECTURA Y COMPONENTES
===========================================

1. INGESTA (document_loader.py)
   - Extrae texto de PDFs con PyMuPDF
   - Soporte OCR opcional con pytesseract
   - Divisi√≥n en chunks con solapamiento

2. EMBEDDINGS (embeddings.py)
   - sentence-transformers (modelo multilingual)
   - Genera vectores de 384 dimensiones
   - Modelo: paraphrase-multilingual-MiniLM-L12-v2

3. B√öSQUEDA VECTORIAL (vector_store.py)
   - FAISS para b√∫squeda eficiente
   - Similitud coseno
   - Persistencia en BD (DocumentChunk)

4. GENERACI√ìN (views.py)
   - Google Gemini 2.0 Flash
   - Prompt engineering con contexto
   - Respuestas fundamentadas en docs

===========================================
MODELOS DE BASE DE DATOS
===========================================

DocumentCollection:
  - Documento subido (PDF)
  - Estado: pending ‚Üí processing ‚Üí indexed
  - Cuenta de p√°ginas y chunks

DocumentChunk:
  - Fragmento de texto (chunk)
  - Embedding vectorial (JSON)
  - Metadata: p√°ginas, √≠ndice, etc.

RAGQuery:
  - Historial de consultas y respuestas
  - Trazabilidad de chunks usados

===========================================
ENDPOINTS API
===========================================

1. SUBIR DOCUMENTO
   POST /rag-chat/api/upload/

   Form-data:
     file: <archivo.pdf>
     title: "Manual de Usuario"

   Respuesta:
   {
     "collection_id": 1,
     "title": "Manual de Usuario",
     "status": "indexed",
     "chunk_count": 45,
     "page_count": 12
   }

   Nota: Procesa el PDF, genera chunks, embeddings
         y los indexa autom√°ticamente.

-------------------------------------------

2. CONSULTAR CON RAG
   POST /rag-chat/api/query/

   Body:
   {
     "collection_id": 1,
     "query": "¬øC√≥mo crear un nuevo empleado?",
     "top_k": 3
   }

   Respuesta:
   {
     "answer": "Para crear un nuevo empleado...",
     "sources": [
       {
         "content": "Fragmento relevante del manual...",
         "page": [5],
         "similarity": 0.892
       }
     ],
     "collection_title": "Manual de Usuario"
   }

   Flujo:
   1. Genera embedding de la pregunta
   2. Busca los 3 chunks m√°s similares en FAISS
   3. Construye prompt con contexto
   4. Llama a Google API para generar respuesta
   5. Guarda en historial

-------------------------------------------

3. LISTAR DOCUMENTOS
   GET /rag-chat/api/documents/

   Respuesta:
   {
     "documents": [
       {
         "id": 1,
         "title": "Manual de Usuario",
         "status": "indexed",
         "page_count": 12,
         "chunk_count": 45,
         "created_at": "2025-11-13T10:00:00",
         "error": null
       }
     ]
   }

-------------------------------------------

4. ELIMINAR DOCUMENTO
   DELETE /rag-chat/api/document/<id>/

   Ejemplo: DELETE /rag-chat/api/document/1/

   Respuesta:
   {
     "message": "Documento 'Manual de Usuario' eliminado"
   }

-------------------------------------------

5. HISTORIAL DE CONSULTAS
   GET /rag-chat/api/history/?limit=20

   Respuesta:
   {
     "history": [
       {
         "id": 15,
         "query": "¬øC√≥mo resetear contrase√±a?",
         "response": "Para resetear la contrase√±a...",
         "collection": "Manual de Usuario",
         "created_at": "2025-11-13T11:30:00"
       }
     ]
   }

===========================================
INSTALACI√ìN Y CONFIGURACI√ìN
===========================================

*** SIGUE ESTOS PASOS EN ORDEN ***

PASO 1: INSTALAR DEPENDENCIAS
------------------------------

Las dependencias ya est√°n en requirements.txt, inst√°lalas:

cd /home/christian/Documents/bar-galileo
pip install pymupdf sentence-transformers faiss-cpu numpy torch

Esto instalar√°:
- pymupdf: Lee PDFs
- sentence-transformers: Genera embeddings (vectores)
- faiss-cpu: B√∫squeda vectorial r√°pida
- torch: Requerido por sentence-transformers

Tiempo estimado: 5-10 minutos (descarga ~1.5GB)

NOTA: Si tienes GPU, instala faiss-gpu en vez de faiss-cpu

PASO 2: MIGRAR BASE DE DATOS
-----------------------------

Crea las tablas en SQLite:

cd bar_galileo
python3 manage.py makemigrations rag_chat
python3 manage.py migrate

Esto crea 3 tablas:
- rag_chat_documentcollection: Documentos subidos
- rag_chat_documentchunk: Fragmentos con embeddings
- rag_chat_ragquery: Historial de consultas

PASO 3: VERIFICAR CONFIGURACI√ìN
--------------------------------

La app ya est√° registrada en:
- INSTALLED_APPS (settings.py)
- URLs (bar_galileo/urls.py) como /rag-chat/

Verifica que GOOGLE_API_KEY est√© en tu .env:
GOOGLE_API_KEY=tu_clave_aqui

PASO 4: REINICIAR SERVIDOR
---------------------------

Si el servidor estaba corriendo, rein√≠cialo para que cargue
los nuevos m√≥dulos.

===========================================
PRIMER USO: INDEXAR UN DOCUMENTO
===========================================

Una vez instalado, el flujo es:

1. SUBIR PDF
------------

curl -X POST http://localhost:8000/rag-chat/api/upload/ \
  -F "file=@/ruta/a/manual_usuario.pdf" \
  -F "title=Manual de Usuario"

¬øQu√© pasa internamente?
a) Se guarda el PDF en media/rag_documents/
b) PyMuPDF extrae el texto de cada p√°gina
c) El texto se divide en chunks de ~500 palabras con solapamiento
d) sentence-transformers genera un vector de 384 n√∫meros por cada chunk
e) Los vectores se guardan en la BD (tabla documentchunk)
f) Status cambia a "indexed"

Tiempo: ~30 segundos para un PDF de 50 p√°ginas

Respuesta esperada:
{
  "collection_id": 1,
  "title": "Manual de Usuario",
  "status": "indexed",
  "chunk_count": 45,
  "page_count": 12
}

Si falla, revisa:
- Que el PDF no est√© corrupto
- Logs del servidor para ver el error

2. HACER CONSULTA
-----------------

curl -X POST http://localhost:8000/rag-chat/api/query/ \
  -H "Content-Type: application/json" \
  -d '{
    "collection_id": 1,
    "query": "¬øC√≥mo crear un nuevo empleado?",
    "top_k": 3
  }'

¬øQu√© pasa internamente?
a) sentence-transformers genera vector de tu pregunta
b) FAISS busca los 3 chunks m√°s similares (b√∫squeda vectorial)
c) Se construye un prompt con esos 3 fragmentos como contexto
d) Google Gemini genera la respuesta bas√°ndose en el contexto
e) Se guarda la query en el historial

Tiempo: ~2-3 segundos

Respuesta esperada:
{
  "answer": "Para crear un nuevo empleado, ve a...",
  "sources": [
    {
      "content": "Fragmento relevante del manual...",
      "page": [5],
      "similarity": 0.892
    },
    ...
  ],
  "collection_title": "Manual de Usuario"
}

===========================================
¬øC√ìMO FUNCIONA LA BASE VECTORIAL?
===========================================

NO ES UNA BASE DE DATOS SEPARADA.

Todo se guarda en tu SQLite/PostgreSQL normal:

1. EMBEDDINGS EN LA BD
-----------------------

Tabla: rag_chat_documentchunk

Cada registro tiene:
- content: "Para crear un empleado, navegue al men√∫..."
- embedding: [0.234, -0.891, 0.445, ..., 0.123] (384 n√∫meros)
- metadata: {"source_pages": [5], "chunk_index": 12}

Los embeddings se guardan como JSON en la columna "embedding".

2. √çNDICE FAISS EN MEMORIA
---------------------------

Cuando haces una consulta:

a) Se cargan TODOS los embeddings de la BD
b) Se construye un √≠ndice FAISS en RAM
c) FAISS hace la b√∫squeda r√°pida (similitud coseno)
d) Retorna los IDs de los chunks m√°s similares
e) Se consultan esos chunks en la BD para obtener el texto

FAISS es solo para B√öSQUEDA R√ÅPIDA, no almacena nada.
Los datos reales est√°n en tu BD de Django.

3. ¬øPOR QU√â FAISS?
-------------------

Comparar tu pregunta con 1000 chunks uno por uno:
‚Üí Lento (~5 segundos)

Comparar con FAISS:
‚Üí R√°pido (~50ms)

FAISS usa algoritmos optimizados para encontrar
los vectores m√°s cercanos sin comparar todos.

===========================================
CONCEPTOS CLAVE EXPLICADOS
===========================================

EMBEDDING / VECTOR
------------------
Una lista de n√∫meros que representa el SIGNIFICADO de un texto.

Ejemplo:
"crear empleado" ‚Üí [0.2, -0.8, 0.4, ..., 0.1]
"agregar trabajador" ‚Üí [0.19, -0.79, 0.41, ..., 0.09]

Estos dos vectores son SIMILARES porque el significado es parecido.

CHUNK
-----
Fragmento de texto. Como tu PDF tiene 50 p√°ginas (mucho),
lo dividimos en pedazos de ~500 palabras.

¬øPor qu√©? Porque Google Gemini tiene l√≠mite de tokens y
queremos darle solo lo relevante, no todo el PDF.

SIMILITUD COSENO
----------------
Mide qu√© tan "parecidos" son dos vectores.

Score 0.9 = muy similares (misma idea)
Score 0.3 = poco similares (temas distintos)

TOP_K
-----
"Dame los K fragmentos m√°s similares"

top_k=3 ‚Üí los 3 mejores chunks
top_k=5 ‚Üí los 5 mejores chunks

M√°s chunks = m√°s contexto pero m√°s lento y m√°s tokens usados.

PROMPT ENGINEERING
------------------
C√≥mo le hablamos a Google Gemini.

Malo:
"Pregunta: ¬øC√≥mo crear empleado?"

Bueno:
"Bas√°ndote en este contexto del manual:
[fragmento 1]
[fragmento 2]
[fragmento 3]

Responde: ¬øC√≥mo crear empleado?
Si no est√° en el contexto, di que no sabes."

===========================================

1. INSTALAR DEPENDENCIAS

   pip install pymupdf sentence-transformers faiss-cpu

   Opcional (OCR para PDFs escaneados):
   pip install pytesseract pillow
   apt-get install tesseract-ocr tesseract-ocr-spa

2. CONFIGURAR .env

   GOOGLE_API_KEY=tu_clave_aqui

3. MIGRAR BASE DE DATOS

   python manage.py makemigrations rag_chat
   python manage.py migrate

4. REGISTRAR EN SETTINGS.PY

   INSTALLED_APPS = [
       ...
       'rag_chat',
   ]

5. AGREGAR URLS (bar_galileo/urls.py)

   urlpatterns = [
       ...
       path('rag-chat/', include(('rag_chat.urls', 'rag_chat'), namespace='rag_chat')),
   ]

===========================================
EJEMPLO DE USO
===========================================

# 1. Subir manual de usuario
curl -X POST http://localhost:8000/rag-chat/api/upload/ \
  -F "file=@manual_usuario.pdf" \
  -F "title=Manual de Usuario v1.0"

# Respuesta: {"collection_id": 1, ...}

# 2. Hacer consulta
curl -X POST http://localhost:8000/rag-chat/api/query/ \
  -H "Content-Type: application/json" \
  -d '{
    "collection_id": 1,
    "query": "¬øC√≥mo crear un nuevo empleado en el sistema?",
    "top_k": 3
  }'

# 3. Ver historial
curl http://localhost:8000/rag-chat/api/history/

===========================================
AJUSTES AVANZADOS
===========================================

CAMBIAR MODELO DE EMBEDDINGS (embeddings.py):

  # En vez de 'multilingual', usa:
  generator = EmbeddingGenerator('large')  # Mejor calidad
  # o
  generator = EmbeddingGenerator('mini')   # M√°s r√°pido

AJUSTAR CHUNK SIZE (document_loader.py):

  chunks = loader.chunk_text(
      pages_data,
      chunk_size=800,  # M√°s contexto por chunk
      overlap=100      # Mayor solapamiento
  )

MEJORAR B√öSQUEDA VECTORIAL (vector_store.py):

  # Para millones de vectores, cambiar a IndexIVFFlat:
  quantizer = faiss.IndexFlatL2(dimension)
  self.index = faiss.IndexIVFFlat(quantizer, dimension, 100)
  self.index.train(training_vectors)

===========================================
MEJORAS FUTURAS
===========================================

‚òê Procesamiento as√≠ncrono con Celery
‚òê Soporte para DOCX, TXT, MD
‚òê Re-ranking con cross-encoders
‚òê Interfaz web interactiva
‚òê Chunking inteligente por secciones
‚òê Cach√© de embeddings frecuentes
‚òê Soporte multimodal (im√°genes en PDFs)
‚òê Fine-tuning del modelo de embeddings

===========================================
TROUBLESHOOTING
===========================================

Error: "sentence-transformers no instalado"
‚Üí pip install sentence-transformers

Error: "FAISS no instalado"
‚Üí pip install faiss-cpu (o faiss-gpu si tienes CUDA)

Documento queda en "processing"
‚Üí Revisar logs para ver el error espec√≠fico
‚Üí Verificar que el PDF no est√© corrupto

Respuestas irrelevantes
‚Üí Aumentar top_k a 5-7
‚Üí Cambiar a modelo de embeddings 'large'
‚Üí Ajustar chunk_size y overlap

OCR muy lento
‚Üí Desactivar OCR si no es necesario
‚Üí O usar GPU con easyocr en vez de pytesseract

===========================================
ESTADO ACTUAL DEL PROYECTO
===========================================

‚úÖ C√ìDIGO CREADO:
- Modelos de BD (DocumentCollection, DocumentChunk, RAGQuery)
- M√≥dulos Python (document_loader, embeddings, vector_store)
- Views con endpoints RESTful
- URLs configuradas
- Admin de Django
- App registrada en INSTALLED_APPS

‚ùå FALTA HACER (T√ö):
- Instalar dependencias (pip install...)
- Crear tablas (makemigrations, migrate)
- Probar con un PDF real
- Ajustar par√°metros si es necesario

üìÅ ARCHIVOS CREADOS:
bar_galileo/rag_chat/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ apps.py
‚îú‚îÄ‚îÄ models.py              ‚Üê Tablas de BD
‚îú‚îÄ‚îÄ admin.py               ‚Üê Django admin
‚îú‚îÄ‚îÄ views.py               ‚Üê 5 endpoints (upload, query, etc)
‚îú‚îÄ‚îÄ urls.py                ‚Üê Rutas
‚îú‚îÄ‚îÄ document_loader.py     ‚Üê Lee PDFs y crea chunks
‚îú‚îÄ‚îÄ embeddings.py          ‚Üê Genera vectores con sentence-transformers
‚îú‚îÄ‚îÄ vector_store.py        ‚Üê B√∫squeda FAISS
‚îî‚îÄ‚îÄ README.txt             ‚Üê Este archivo

===========================================
DIAGRAMA DE FLUJO COMPLETO
===========================================

1. INDEXACI√ìN (una vez por documento)
--------------------------------------

   [PDF]
     ‚Üì
   document_loader.py
     ‚Üí Lee p√°ginas
     ‚Üí Divide en chunks de 500 palabras
     ‚Üì
   embeddings.py
     ‚Üí Genera vector por cada chunk
     ‚Üì
   [Base de Datos]
     ‚Üí Guarda chunks + vectores

2. CONSULTA (cada vez que preguntas)
-------------------------------------

   [Pregunta del usuario]
     ‚Üì
   embeddings.py
     ‚Üí Genera vector de la pregunta
     ‚Üì
   vector_store.py (FAISS)
     ‚Üí Busca chunks similares en BD
     ‚Üí Retorna los 3 m√°s parecidos
     ‚Üì
   views.py
     ‚Üí Construye prompt con contexto
     ‚Üí Llama a Google Gemini API
     ‚Üì
   [Respuesta fundamentada]
     + Fuentes (p√°ginas)

===========================================
RESUMEN PARA NO PROGRAMADORES
===========================================

¬øQu√© problema resuelve esto?

ANTES:
- Usuario: "¬øC√≥mo hacer X?"
- T√∫: "Busca en el manual en la p√°gina... no s√© cu√°l"
- Usuario pierde tiempo buscando

DESPU√âS:
- Usuario: "¬øC√≥mo hacer X?"
- Sistema: "Seg√∫n el manual (p√°g 5): Para hacer X..."
- Usuario tiene respuesta instant√°nea con fuente

¬øC√≥mo lo hace?

1. Subes el PDF del manual (1 vez)
2. Sistema lo "lee" y lo convierte en n√∫meros
3. Usuario pregunta
4. Sistema busca qu√© parte del manual responde
5. Le pasa esa parte a la IA (Google Gemini)
6. IA responde bas√°ndose en tu manual, no inventando

===========================================
NOTAS IMPORTANTES
===========================================

‚úì Archivos subidos se guardan en media/rag_documents/
‚úì Embeddings se guardan en BD (JSON) para persistencia
‚úì FAISS se reconstruye en memoria al iniciar
‚úì Google API tiene l√≠mite de tokens (~30K para Gemini)
‚úì Para producci√≥n, considera usar Celery para procesamiento
‚úì El modelo multilingual funciona bien en espa√±ol
